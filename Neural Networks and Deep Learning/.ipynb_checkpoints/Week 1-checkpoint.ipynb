{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Welcome</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning has already transformed traditional internet bussinesses like serach and advertising, but also enabling brand new products ways to help people. Better healthcare such as reading x-rays, precision agriculture, self dcriving cars etc. Here you'll learn the tools for such applications and be able to confidently play a role in creating an AI powered society. \n",
    "\n",
    "AI is new electricity, just like electricity transformed transportation, manufacturing, healthcare, communications, AI will do the same. \n",
    "\n",
    "First course Neural Networks and Deep Learning teaches you to build and train a deep neural network on data, including images. Second course Improving Deep Neural Networks teaches practical aspects like hyperparameter tuning, regularization, optimization, demystify pro techniques. Third course Structuring your Machine Learning Project will cover strategy which has changed in era of DL, the way to split train/dev/test has changed. What if train test come from different dists, how to deal with that, etc. Also end-to-end DL and when should/shouldn't use it. This course relatively unique, his hard won lessons building/shipping ML products, largely not taught in uni ML. Fourth course Convolutional Neural Networks, build CNNs and how they work for images. Fifth and final course is Natural Language Processing, teaches sequence models RNN LSTM etc. when to use how apply like speech, music gen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>What is a Neural Network?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple housing prediction with size of house sq ft as input, and want predict output price as function of size. Could fit straight line with lin reg, or maybe could angle line at beginning zero slope (prices never negative) up until a point, then fixed slope after it. This fancier line would be a simple neural network. Input size x, output price y; single neuron implements that fancier line. This function that is zero slope up until then fixed after a point comes from relu (rectified linear unit). Can have a neuron per input so several neurons a layer stack them in layers, akin to lego bricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![housingprice](../images/NeuralNetworksandDeepLearning/housingprice.png)\n",
    "\n",
    "instead of just size, say you also know other features like # bedrooms, and say with size and #bedrooms together you have family size in layer 2, then with zip/postal code you make walkability in layer 2 and with zip & wealth you make a \"school\" neuron in layer 2. That's 3 neurons in layer 2, and those 3 go together into price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 2 is a hidden layer, and these hidden layer neurons (just now hand made & labelled in the smaller draft) are part of magic of NN, these weights are figured out as part of itself in the final (larger) NN graph in below picture if you give all inputs to each hidden layer neuron. It could then decide to have some inputs have zero weight in some hidden neurons. Remarkable thing about NN is given enough $x$ and $y$, they're very good at figuring out function from $x$ to $y$ using all their weights especially hiddens. Great for these supervised learning type problems.\n",
    "\n",
    "![firstNN](../images/NeuralNetworksandDeepLearning/firstNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Supervised Learning with Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot of hype about NNs, bunch justified given results, but turns out almost all in supervised learning. So just function mapping input $x$ to output $y$.\n",
    "Layer 2 is a hidden layer, and these hidden layer neurons (just now hand made & labelled in the smaller draft) are part of magic of NN, these weights are figured out as part of itself in the final (larger) NN graph in below picture if you give all inputs to each hidden layer neuron. It could then decide to have some inputs have zero weight in some hidden neurons. Remarkable thing about NN is given enough x and y, they're very good at figuring out function from $x$ to $y$ using all their weights especially hiddens. Great for these supervised learning type problems.\n",
    "\n",
    "![supervised](../images/NeuralNetworksandDeepLearning/supervised.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all these problems NNs have excelled. Most lucrative by far has been advertising. Go figure still 93+% Google revenue is from AdSense. By taking ad and user info as input and being able to tell whether you will click on it, then show people ads they want to click has been incredibly lucrative application at multiple companies. \n",
    "\n",
    "Side note RNN category are all sequence data.\n",
    "\n",
    "![typesNN](../images/NeuralNetworksandDeepLearning/typesNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May also heard of structured data and unstructured. Structured is tabular, rows are training examples columns are input features and output for each example. Like size, # bedrooms; price. Or User age, ad id; click. In contrast unstructured is like audio(features are waveform), images (features are pixels), text (features are words/char). \n",
    "\n",
    "Historically much harder computers to do unstructured than structured, but humans evolved to do just that. So one of exciting things of rise of NN is much better / more possible at unstructured than even few years ago. So unstructured just more in media since humans default but opposite for comp. \n",
    "\n",
    "But, most money short term NN is in structured such as advertising system, product recommendations. \n",
    "\n",
    "Techniques in this course will go over both structured and un-, but for purpose explaining algorithms will use unstructured. Should look both for find applications/uses though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Why is Deep Learning Taking Off?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the math for it has been around for decades, why it's working well now suddenly is because scale of compute (cheap fast GPU), scaled of data explosion and improvements in optimization/activation algs (sigmoid to relu switch since sigmoid gradient nearly zero slow learning at large vals).\n",
    "\n",
    "Traditional learning algs taper off like log base 10+ for improvement from increase in size m of training set of (x,y). Small NN some improvement over trad, medium even some more, but large NN almost just keeps getting better and better with more data linear all way. When small training set doesn't matter much trad, or small/med/large NN. \n",
    "\n",
    "![tradvsNN](../images/NeuralNetworksandDeepLearning/tradvsNN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a huge difference in productivitiy if can do a loop of iteration in 10 min vs a day or a month. The great increase in process of result feedback for next iteration has been super for practitioners researchers alike much more likely to find NN that works for you. This speed up has been harnessed for nonstop progress in optimization/activation/etc. algorithms. The three forces are only growing, so more and more larger NN keep on getting better looks very optimistic. \n",
    "\n",
    "![scaleitshelp](../images/NeuralNetworksandDeepLearning/scaleitshelp.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
